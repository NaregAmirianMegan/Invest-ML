{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model creation functions\n",
    "def make_dense_layer(prev_layer, n_inputs, n_nodes, activation_fn, layer_name):\n",
    "    W = tf.Variable(tf.random.normal([n_inputs, n_nodes]), name=layer_name)\n",
    "    b = tf.Variable(tf.random.normal([n_nodes]), name=layer_name)\n",
    "    return activation_fn(tf.add(tf.matmul(prev_layer, W), b))\n",
    "\n",
    "def make_dense_nn(scope, n_inputs, n_outputs, h_layer_node_dict, loss_fn, lr=1e-4):\n",
    "    with tf.variable_scope(scope):\n",
    "        x = tf.placeholder(tf.float32, [None, n_inputs], name=\"x\")\n",
    "        y = tf.placeholder(tf.float32, [None, n_outputs], name=\"y\")\n",
    "        \n",
    "        prev_layer = x\n",
    "        prev_n_inputs = n_inputs\n",
    "        for layer_name, layer_nodes in h_layer_node_dict.items():\n",
    "            prev_layer = make_dense_layer(prev_layer, prev_n_inputs, layer_nodes, tf.nn.relu, layer_name)\n",
    "            prev_n_inputs = layer_nodes\n",
    "        \n",
    "        output = make_dense_layer(prev_layer, prev_n_inputs, n_outputs, tf.keras.activations.linear, \"output\")\n",
    "        loss = tf.reduce_mean(loss_fn(y, output))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "        \n",
    "        return x, y, output, loss, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct models\n",
    "from_model = {}\n",
    "to_model = {}\n",
    "from_model['x'], from_model['y'], from_model['output'], from_model['loss'], from_model['train_op'] = make_dense_nn(\"from_model\", 2, 1, {'h1': 5}, tf.losses.mean_squared_error)\n",
    "to_model['x'], to_model['y'], to_model['output'], to_model['loss'], to_model['train_op'] = make_dense_nn(\"to_model\", 2, 1, {'h1': 5}, tf.losses.mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define constants\n",
    "train_steps = 20000\n",
    "x_batch = np.array([[0, 0],\n",
    "                    [0, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1]])\n",
    "y_batch = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model training\n",
    "def train(model, train_steps, x_batch, y_batch, sess):\n",
    "    for step in range(train_steps):\n",
    "        _, loss = sess.run([model['train_op'], model['loss']], feed_dict={model['x']: x_batch, model['y']: y_batch})\n",
    "        if step % 1000 == 0:\n",
    "            print(\"Loss:\", loss)\n",
    "            print(\"Predictions:\", np.round(sess.run(from_model['output'], feed_dict={from_model['x']: x_batch}), decimals=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define copy function\n",
    "def copy_weights(from_model_scope, to_model_scope, sess):\n",
    "    for from_model_var, to_model_var in zip(tf.trainable_variables(from_model_scope), tf.trainable_variables(to_model_scope)):\n",
    "        frm = from_model_var.eval(session=sess)\n",
    "        to = to_model_var.eval(session=sess)\n",
    "        np.copyto(to, frm)\n",
    "        to_model_var.load(to, session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.728973\n",
      "Predictions: [[-1.]\n",
      " [ 0.]\n",
      " [-2.]\n",
      " [-1.]]\n",
      "Loss: 2.0766695\n",
      "Predictions: [[-0.]\n",
      " [ 1.]\n",
      " [-2.]\n",
      " [-1.]]\n",
      "Loss: 1.3339895\n",
      "Predictions: [[ 0.]\n",
      " [ 1.]\n",
      " [-1.]\n",
      " [-0.]]\n",
      "Loss: 0.9824332\n",
      "Predictions: [[ 0.]\n",
      " [ 1.]\n",
      " [-1.]\n",
      " [ 0.]]\n",
      "Loss: 0.7130402\n",
      "Predictions: [[ 0.]\n",
      " [ 1.]\n",
      " [-1.]\n",
      " [ 0.]]\n",
      "Loss: 0.4969582\n",
      "Predictions: [[ 0.]\n",
      " [ 1.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "Loss: 0.3346431\n",
      "Predictions: [[ 0.]\n",
      " [ 1.]\n",
      " [-0.]\n",
      " [ 0.]]\n",
      "Loss: 0.21862866\n",
      "Predictions: [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Loss: 0.16387601\n",
      "Predictions: [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Loss: 0.14482836\n",
      "Predictions: [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Loss: 0.13820386\n",
      "Predictions: [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Loss: 0.13413206\n",
      "Predictions: [[-0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n",
      "Loss: 0.13071024\n",
      "Predictions: [[-0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n",
      "Loss: 0.1278908\n",
      "Predictions: [[-0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n",
      "Loss: 0.12559038\n",
      "Predictions: [[-0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n",
      "Loss: 0.12324626\n",
      "Predictions: [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Loss: 0.11873431\n",
      "Predictions: [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Loss: 0.10804272\n",
      "Predictions: [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Loss: 0.090050526\n",
      "Predictions: [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Loss: 0.06714769\n",
      "Predictions: [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "#train from model, copy weights, and test to model\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#train from model\n",
    "train(from_model, train_steps, x_batch, y_batch, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.95704967 -0.33637187 -0.6557394  -0.49554834 -0.29681146]\n",
      " [ 0.67364347  0.8883007  -0.8154123  -0.8419048  -1.1889317 ]]\n",
      "[ 4.4657465e-02 -7.1046400e-01  6.5411484e-01  4.9364975e-01\n",
      " -6.8340611e-05]\n",
      "[[-0.6138715]\n",
      " [ 0.8406372]\n",
      " [-0.9984872]\n",
      " [-1.2344583]\n",
      " [ 2.2588663]]\n",
      "[1.2913297]\n",
      "-------------------------------------\n",
      "[[-1.6487391  -1.2189791   0.11233269  1.0254226   0.63858086]\n",
      " [ 0.28302047 -0.05176385 -0.39390084 -0.59137017  0.9084476 ]]\n",
      "[ 0.29813087 -0.66631234  0.5445818  -1.349654   -0.98038894]\n",
      "[[-0.68178093]\n",
      " [ 0.86751264]\n",
      " [-0.1132599 ]\n",
      " [-0.5619872 ]\n",
      " [-0.6716718 ]]\n",
      "[-0.19542968]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[ 0.95704967 -0.33637187 -0.6557394  -0.49554834 -0.29681146]\n",
      " [ 0.67364347  0.8883007  -0.8154123  -0.8419048  -1.1889317 ]]\n",
      "[ 4.4657465e-02 -7.1046400e-01  6.5411484e-01  4.9364975e-01\n",
      " -6.8340611e-05]\n",
      "[[-0.6138715]\n",
      " [ 0.8406372]\n",
      " [-0.9984872]\n",
      " [-1.2344583]\n",
      " [ 2.2588663]]\n",
      "[1.2913297]\n",
      "-------------------------------------\n",
      "[[ 0.95704967 -0.33637187 -0.6557394  -0.49554834 -0.29681146]\n",
      " [ 0.67364347  0.8883007  -0.8154123  -0.8419048  -1.1889317 ]]\n",
      "[ 4.4657465e-02 -7.1046400e-01  6.5411484e-01  4.9364975e-01\n",
      " -6.8340611e-05]\n",
      "[[-0.6138715]\n",
      " [ 0.8406372]\n",
      " [-0.9984872]\n",
      " [-1.2344583]\n",
      " [ 2.2588663]]\n",
      "[1.2913297]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predictions: [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "#copy weights test\n",
    "for var in tf.trainable_variables(\"from_model\"):\n",
    "    print(var.eval(session=sess))\n",
    "    \n",
    "print(\"-------------------------------------\")\n",
    "    \n",
    "for var in tf.trainable_variables(\"to_model\"):\n",
    "    print(var.eval(session=sess))\n",
    "    \n",
    "\n",
    "copy_weights(\"from_model\", \"to_model\", sess)\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "#test to model\n",
    "for var in tf.trainable_variables(\"from_model\"):\n",
    "    print(var.eval(session=sess))\n",
    "    \n",
    "print(\"-------------------------------------\")\n",
    "    \n",
    "for var in tf.trainable_variables(\"to_model\"):\n",
    "    print(var.eval(session=sess))\n",
    "    \n",
    "    \n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(\"Predictions:\", np.round(sess.run(to_model['output'], feed_dict={to_model['x']: x_batch}), decimals=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
